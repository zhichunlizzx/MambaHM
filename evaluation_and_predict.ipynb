{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local1/zzx/conda/envs/LongSeq/lib/python3.9/site-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
      "/local1/zzx/conda/envs/LongSeq/lib/python3.9/site-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout):\n",
      "/local1/zzx/conda/envs/LongSeq/lib/python3.9/site-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/local1/zzx/conda/envs/LongSeq/lib/python3.9/site-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/local1/zzx/conda/envs/LongSeq/lib/python3.9/site-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
      "/local1/zzx/conda/envs/LongSeq/lib/python3.9/site-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/local1/zzx/conda/envs/LongSeq/lib/python3.9/site-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
      "/local1/zzx/conda/envs/LongSeq/lib/python3.9/site-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = 0\n",
      "Torch sees 1 GPUs\n",
      "Device 0 name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Copyright 2023 Z Zhang\n",
    "\n",
    "# BioSeq2Seq, Version 1.0;\n",
    "# you may not use this file except in compliance with the License.\n",
    "# Use of this code requires following originality guidelines\n",
    "# and declaring the source of the code.\n",
    "# email:zhichunli@mail.dlut.edu.cn\n",
    "# =========================================================================\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "from re import I\n",
    "import numpy as np\n",
    "from model.model_MambaHM import MambaHM\n",
    "from utils.dataloader import RD_dataloader\n",
    "from utils.functions import bw_2_chromosome_size\n",
    "from torch.utils.data import DataLoader\n",
    "from numpy.core.multiarray import scalar\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from torchmetrics import Metric\n",
    "from einops import rearrange\n",
    "from operator import itemgetter\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "DEVICE=torch.device(\"cuda:0\")\n",
    "\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(\"Torch sees\", torch.cuda.device_count(), \"GPUs\")\n",
    "print(\"Device 0 name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "evaluation\n",
    "'''\n",
    "def write_predicted_result(\n",
    "                        results,\n",
    "                        out_path,\n",
    "                        chr_length,\n",
    "                        target_list,\n",
    "                        reference_genome_idx,\n",
    "                        seq_length=114688,\n",
    "                        window_size=128,\n",
    "                        ):\n",
    "    \"\"\" \n",
    "    Write result to bigwig file\n",
    "\n",
    "    Args:\n",
    "        results: predicted result, {chr:[{start:xx, end:xx, result:xx}]}\n",
    "        out_path: output path\n",
    "        chr_length: chromosome length\n",
    "        target_list: target sequencing data list\n",
    "        reference_genome_idx: reference genome idx\n",
    "\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    seq_length = seq_length\n",
    "    target_length = seq_length // window_size\n",
    "    \n",
    "    for j in range(len(target_list)):\n",
    "            if os.path.isfile(os.path.join(out_path, target_list[j] + '.bedgraph')):\n",
    "                os.remove(os.path.join(out_path, target_list[j] + '.bedgraph'))\n",
    "\n",
    "    for chr in results:\n",
    "        chr_result = results[chr]\n",
    "        chr_result = sorted(chr_result, key=itemgetter('start'))\n",
    "        for j in range(len(target_list)):\n",
    "            with open(os.path.join(out_path, target_list[j] + '.bedgraph'), 'a') as w_obj:\n",
    "                # assign 0 to the area not covered by the sample\n",
    "                if chr_result[0]['start'] > 0:\n",
    "                    w_obj.write(chr + '\\t' + str(0) + '\\t' + str(chr_result[0]['start']) + '\\t' + str(0) + '\\n')\n",
    "\n",
    "                # write predict result\n",
    "                last_end = 0\n",
    "                for item in chr_result:\n",
    "                    if item['start'] >= last_end: \n",
    "                        for i in range(target_length):\n",
    "                            start = item['start'] + i * window_size\n",
    "                            end = start + window_size\n",
    "                            w_obj.write(chr + '\\t' + str(start) + '\\t' + str(end) + '\\t' + str(item['predicted'][i][j]) + '\\n')\n",
    "                    else:\n",
    "                        print(item)\n",
    "                        gap_h = last_end - item['start']\n",
    "                        h_start = gap_h // window_size\n",
    "                        w_obj.write(chr + '\\t' + str(last_end) + '\\t' + str(item['start'] + window_size * (h_start+1)) + '\\t' + str(item['predicted'][h_start][j]) + '\\n')\n",
    "                        for i in range(h_start+1, target_length):\n",
    "                            start = item['start'] + i * window_size\n",
    "                            end = start + window_size \n",
    "                            w_obj.write(chr + '\\t' + str(start) + '\\t' + str(end) + '\\t' + str(item['predicted'][i][j]) + '\\n')\n",
    "                last_end = item['end']\n",
    "\n",
    "                # assign 0 to the area not covered by the sample\n",
    "                if chr_result[-1]['end'] < chr_length[chr]:\n",
    "                    w_obj.write(chr + '\\t' + str(chr_result[-1]['end']) + '\\t' + str(chr_length[chr]) + '\\t' + str(0) + '\\n')\n",
    "\n",
    "    # bedgraph to bigwig\n",
    "    for j in range(len(target_list)):\n",
    "        bed_path = os.path.join(out_path, target_list[j] + '.bedgraph')\n",
    "        bedgraph_path_sorted = os.path.join(out_path, target_list[j] + '_sorted.bedgraph')\n",
    "        cmd_bedSort = 'sort-bed ' + bed_path + ' > ' + bedgraph_path_sorted\n",
    "        p = subprocess.Popen(cmd_bedSort, shell=True)\n",
    "        p.wait()\n",
    "\n",
    "        bw_path = os.path.join(out_path, target_list[j] + '.bw')\n",
    "\n",
    "        cmd = ['bedGraphToBigWig', bedgraph_path_sorted, reference_genome_idx, bw_path]\n",
    "        subprocess.call(cmd)\n",
    "\n",
    "        cmd_rm = ['rm', '-f', bed_path]\n",
    "        subprocess.call(cmd_rm)\n",
    "\n",
    "        cmd_rm = ['rm', '-f', bedgraph_path_sorted]\n",
    "        subprocess.call(cmd_rm)\n",
    "\n",
    "    return True\n",
    "\n",
    "class MeanPearsonCorrCoefPerChannel(Metric):\n",
    "    is_differentiable: Optional[bool] = False\n",
    "    higher_is_better: Optional[bool] = True\n",
    "    def __init__(self, n_channels:int, dist_sync_on_step=False):\n",
    "        \"\"\"Calculates the mean pearson correlation across channels aggregated over regions\"\"\"\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.reduce_dims=(0, 1)\n",
    "        self.add_state(\"product\", default=torch.zeros(n_channels, dtype=torch.float32), dist_reduce_fx=\"sum\", )\n",
    "        self.add_state(\"true\", default=torch.zeros(n_channels, dtype=torch.float32), dist_reduce_fx=\"sum\", )\n",
    "        self.add_state(\"true_squared\", default=torch.zeros(n_channels, dtype=torch.float32), dist_reduce_fx=\"sum\", )\n",
    "        self.add_state(\"pred\", default=torch.zeros(n_channels, dtype=torch.float32), dist_reduce_fx=\"sum\", )\n",
    "        self.add_state(\"pred_squared\", default=torch.zeros(n_channels, dtype=torch.float32), dist_reduce_fx=\"sum\", )\n",
    "        self.add_state(\"count\", default=torch.zeros(n_channels, dtype=torch.float32), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.shape == target.shape\n",
    "\n",
    "        self.product += torch.sum(preds * target, dim=self.reduce_dims)\n",
    "        self.true += torch.sum(target, dim=self.reduce_dims)\n",
    "        self.true_squared += torch.sum(torch.square(target), dim=self.reduce_dims)\n",
    "        self.pred += torch.sum(preds, dim=self.reduce_dims)\n",
    "        self.pred_squared += torch.sum(torch.square(preds), dim=self.reduce_dims)\n",
    "        self.count += torch.sum(torch.ones_like(target), dim=self.reduce_dims)\n",
    "\n",
    "    def compute(self):\n",
    "        true_mean = self.true / self.count\n",
    "        pred_mean = self.pred / self.count\n",
    "\n",
    "        covariance = (self.product\n",
    "                    - true_mean * self.pred\n",
    "                    - pred_mean * self.true\n",
    "                    + self.count * true_mean * pred_mean)\n",
    "\n",
    "        true_var = self.true_squared - self.count * torch.square(true_mean)\n",
    "        pred_var = self.pred_squared - self.count * torch.square(pred_mean)\n",
    "        tp_var = torch.sqrt(true_var) * torch.sqrt(pred_var)\n",
    "        correlation = covariance / tp_var\n",
    "        return correlation\n",
    "    \n",
    "\n",
    "def convert_resolution(target, window_width, aim_resolution):\n",
    "    k = aim_resolution // window_width\n",
    "    target = rearrange(target, 'b (r n) d -> b r n d', n = k)\n",
    "    target = torch.mean(target, dim=2)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_genome_file = 'hg19.fa'\n",
    "sequence_data_file = [\n",
    "                        [\n",
    "                        ['k562.bw'],\n",
    "                         ]\n",
    "                        ]\n",
    "\n",
    "target_seq_file = [\n",
    "    # K562\n",
    "    ['H3k122ac.bigWig',\n",
    "    'H3k4me1.bigWig', \n",
    "    'H3k4me2.bigWig', \n",
    "    'H3k4me3.bigWig', \n",
    "    'H3k27ac.bigWig', \n",
    "    'H3k27me3.bigWig', \n",
    "    'H3k36me3.bigWig', \n",
    "    'H3k9ac.bigWig', \n",
    "    'H3k9me3.bigWig', \n",
    "    'H4k20me1.bigWig', ]\n",
    "    ]\n",
    "\n",
    "validation_samples = np.loadtxt('MambaHM/samples/test.bed', dtype=str, delimiter='\\t')\n",
    "\n",
    "model_dir = 'MambaHM/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231016/2975001650.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=DEVICE    )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.serialization.add_safe_globals([scalar])\n",
    "window_width = 16\n",
    "checkpoint_path = os.path.join(model_dir, 'model.pth')\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE    )\n",
    "\n",
    "model = MambaHM(channels=384,\n",
    "            num_heads=8,\n",
    "            num_transformer_layers=11,\n",
    "            pooling_type='max',\n",
    "            output_channels=len(target_seq_file[0]),\n",
    "            target_length=7168,\n",
    "            device=DEVICE\n",
    "            ).to(DEVICE)\n",
    "\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [03:01,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7497\n",
      "0.8404\n",
      "0.9087\n",
      "0.9123\n",
      "0.8373\n",
      "0.7186\n",
      "0.8176\n",
      "0.8994\n",
      "0.7963\n",
      "0.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eva_data_loader = RD_dataloader(validation_samples,\n",
    "                        reference_genome_file,\n",
    "                        sequence_data_file,\n",
    "                        target_seq_file,\n",
    "                        window_width=window_width,\n",
    "                        extend=40960,  \n",
    "                        nan=0,\n",
    "                        valid=True,\n",
    "                        rc=False,\n",
    "                        )\n",
    "eva_dataset = DataLoader(dataset=eva_data_loader, batch_size=4, shuffle=True)\n",
    "\n",
    "metric = MeanPearsonCorrCoefPerChannel(n_channels=10)\n",
    "model.eval()\n",
    "# model.set_eval()\n",
    "with torch.no_grad():\n",
    "    for i, eva_data_item in tqdm(enumerate(eva_dataset), ascii=True):\n",
    "        # if i > 60:\n",
    "        #     break\n",
    "        dna = eva_data_item[0].to(DEVICE)\n",
    "        seq = eva_data_item[1].to(DEVICE)\n",
    "        target = eva_data_item[2].to(DEVICE)\n",
    "        predicted = model(dna, seq).to(DEVICE)\n",
    "        target_resolution = convert_resolution(target, window_width, 1024).detach().cpu()\n",
    "        predicted_resolution = convert_resolution(predicted, window_width, 1024).detach().cpu()\n",
    "\n",
    "        target_t = target_resolution\n",
    "        predicted_t = predicted_resolution\n",
    "        metric.update(target_t, predicted_t)\n",
    "\n",
    "for pearson in metric.compute().numpy():\n",
    "    print(round(pearson, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [01:12,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed. Results saved to /local1/zzx/code/LongSeq/MambaHM/test\n"
     ]
    }
   ],
   "source": [
    "samples = np.loadtxt('MambaHM/samples/pred_chr22.bed', dtype=str, delimiter='\\t')\n",
    "eva_data_loader = RD_dataloader(samples,\n",
    "                        reference_genome_file,\n",
    "                        sequence_data_file,\n",
    "                        target_seq_file,\n",
    "                        window_width=window_width,\n",
    "                        extend=40960,  \n",
    "                        nan=0,\n",
    "                        valid=True,\n",
    "                        rc=False,\n",
    "                        )\n",
    "eva_dataset = DataLoader(dataset=eva_data_loader, batch_size=1, shuffle=False)\n",
    "\n",
    "target_list = [\n",
    "            'H3k122ac',\n",
    "            'H3k4me1',\n",
    "            'H3k4me2',\n",
    "            'H3k4me3',\n",
    "            'H3k27ac',\n",
    "            'H3k27me3',\n",
    "            'H3k36me3',\n",
    "            'H3k9ac',\n",
    "            'H3k9me3',\n",
    "            'H4k20me1',\n",
    "            ]\n",
    "out_path = 'MambaHM/test'\n",
    "results = {}\n",
    "chrom_size = bw_2_chromosome_size(sequence_data_file[0][0][0])\n",
    "\n",
    "chr_length = {}\n",
    "for chr in np.unique(samples[:, 0]):\n",
    "    results[chr] = []\n",
    "    chr_length[chr] = chrom_size[chr][0][1]\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "# chromosome length file\n",
    "reference_genome_idx = os.path.join(out_path, 'idx.fai')\n",
    "with open(reference_genome_idx, 'w') as w_obj:\n",
    "    for chr in chrom_size:\n",
    "        w_obj.write(chr + '\\t' + str(chrom_size[chr][0][1]) + '\\n')\n",
    "\n",
    "metric = MeanPearsonCorrCoefPerChannel(n_channels=10)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, eva_data_item in tqdm(enumerate(eva_dataset), ascii=True):\n",
    "        result = {}\n",
    "        if i > len(samples):\n",
    "            break\n",
    "        dna = eva_data_item[0].to(DEVICE)\n",
    "        seq = eva_data_item[1].to(DEVICE)\n",
    "        target = eva_data_item[-1].to(DEVICE)\n",
    "        predicted = model(dna, seq).to(DEVICE)\n",
    "\n",
    "        predict_cpu = predicted.detach().cpu()\n",
    "        result['chr'] = samples[i][0]\n",
    "        result['start'] = int(samples[i][1])\n",
    "        result['end'] = int(samples[i][2])\n",
    "        result['predicted'] = predict_cpu[0].numpy()\n",
    "        results[result['chr']].append(result)\n",
    "\n",
    "write_down = write_predicted_result(results, out_path, chr_length, target_list, reference_genome_idx, seq_length=114688, window_size=16)\n",
    "\n",
    "print('Prediction completed. Results saved to %s' % out_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LongSeq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
